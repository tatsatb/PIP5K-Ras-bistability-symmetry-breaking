{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "741ab895",
   "metadata": {},
   "source": [
    "## Intialize the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a46ba4f90ed6849a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T22:55:27.047852Z",
     "start_time": "2025-05-23T22:55:26.603735Z"
    }
   },
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import os\n",
    "from shutil import which\n",
    "import glob\n",
    "import  pandas as pd\n",
    "\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.ion()\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('TkAgg')  # Force Tk backend\n",
    "\n",
    "%matplotlib tk\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb97b21951d1339",
   "metadata": {},
   "source": [
    "## Load the separated channels into numpy arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:40:22.096057Z",
     "start_time": "2025-05-20T22:40:21.996131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.close(\"all\")\n",
    "\n",
    "# Load the tiff file and save the images as numpy arrays\n",
    "Current_folder = os.getcwd()\n",
    "\n",
    "IMAGE_PATH = \"Image-after\"\n",
    "\n",
    "full_path_to_load = f\"/media/tatsatb/Images/{IMAGE_PATH}/{IMAGE_PATH}.tif\"\n",
    "\n",
    "\n",
    "Parent_Dir = os.path.dirname(full_path_to_load)\n",
    "Path_to_Save = os.path.join(Parent_Dir, \"Image_saved_as_MATRIX\")\n",
    "os.makedirs(Path_to_Save, exist_ok=True)\n",
    "\n",
    "tiff_file = full_path_to_load\n",
    "info = tifffile.TiffFile(tiff_file)\n",
    "\n",
    "n_images = len(info.pages)\n",
    "cols = info.pages[0].shape[1]\n",
    "rows = info.pages[0].shape[0]\n",
    "\n",
    "\n",
    "n_frames = int(n_images / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1417881",
   "metadata": {},
   "source": [
    "## Load the image, select the frames, and save separate channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dafc91d7b1850d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:40:22.882147Z",
     "start_time": "2025-05-20T22:40:22.149429Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "I_tmp = np.zeros((rows, cols, n_images), dtype=np.uint16)\n",
    "\n",
    "initial = 1\n",
    "\n",
    "\n",
    "full_path_red_channel_data = os.path.join(Path_to_Save, 'Red_channel.npy')\n",
    "full_path_DIC_channel_data = os.path.join(Path_to_Save, 'DIC_channel.npy')\n",
    "\n",
    "\n",
    "for i in range(0, n_images, 1):\n",
    "    tmp = tifffile.imread(tiff_file, key=i)\n",
    "    I_tmp[:, :, i] = tmp\n",
    "\n",
    "\n",
    "DIC_channel = I_tmp[:, :, 1::2]\n",
    "Red_channel = I_tmp[:, :, 0::2]\n",
    "\n",
    "\n",
    "np.save(full_path_red_channel_data, Red_channel)\n",
    "np.save(full_path_DIC_channel_data, DIC_channel)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax1 = plt.gca()\n",
    "plt.imshow(Red_channel[:, :, 0], cmap='viridis')\n",
    "ax1.grid(False)\n",
    "plt.colorbar()\n",
    "plt.title('Channel Red - First Frame')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "ax3 = plt.gca()\n",
    "plt.imshow(DIC_channel[:, :, 0], cmap='viridis')\n",
    "ax3.grid(False)\n",
    "plt.colorbar()\n",
    "plt.title('Channel DIC - First Frame')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ad68ca",
   "metadata": {},
   "source": [
    "## View the Images, select a polygon that encompasses the cell, and create a mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5d21dac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:16.830552Z",
     "start_time": "2025-05-20T22:40:22.942391Z"
    }
   },
   "outputs": [],
   "source": [
    "from roipoly import RoiPoly\n",
    "from matplotlib.path import Path\n",
    "plt.ion()\n",
    "\n",
    "\n",
    "def enhance_contrast(frame, low_in=2, high_in=98):\n",
    "\n",
    "    # Convert to float for processing\n",
    "    frame_float = frame.astype(float)\n",
    "\n",
    "    # Calculate percentiles for contrast stretching\n",
    "    low_val = np.percentile(frame_float, low_in)\n",
    "    high_val = np.percentile(frame_float, high_in)\n",
    "\n",
    "    # Clip and stretch contrast\n",
    "    adjusted = np.clip(frame_float, low_val, high_val)\n",
    "    adjusted = ((adjusted - low_val) * 255 / (high_val - low_val))\n",
    "\n",
    "    # Ensure uint8 output\n",
    "    return np.clip(adjusted, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def process_single_channel(frame, frame_num):\n",
    "    # Enhance contrast for display\n",
    "    frame_display = enhance_contrast(frame)\n",
    "\n",
    "    attempt = 1\n",
    "    while True:\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(111)\n",
    "        ax.grid(False)\n",
    "        ax.imshow(frame_display, cmap='gray')\n",
    "        ax.set_title(f'Draw ROI - Frame {frame_num+1} - Attempt {attempt}')\n",
    "\n",
    "        try:\n",
    "            roi = RoiPoly(fig=fig, ax=ax, color='r')\n",
    "            plt.show(block=True)\n",
    "\n",
    "            # Verify ROI points exist\n",
    "            if hasattr(roi, 'x') and len(roi.x) > 2:\n",
    "                ny, nx = frame.shape\n",
    "                poly_verts = [(roi.x[i], roi.y[i]) for i in range(len(roi.x))]\n",
    "                x, y = np.meshgrid(np.arange(nx), np.arange(ny))\n",
    "                xy = np.vstack((x.flatten(), y.flatten())).T\n",
    "\n",
    "                # Create path for mask\n",
    "                path = Path(poly_verts)\n",
    "                mask = path.contains_points(xy).reshape(frame.shape)\n",
    "\n",
    "                processed = frame.copy()\n",
    "                processed[~mask] = 0\n",
    "                plt.close(fig)\n",
    "                return processed, mask\n",
    "            else:\n",
    "                print(\"Invalid ROI - please try again\")\n",
    "                plt.close(fig)\n",
    "                attempt += 1\n",
    "                continue\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"Process interrupted by user. Exiting...\")\n",
    "            plt.close(fig)\n",
    "            return None, None\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"ROI Error: {str(e)}\")\n",
    "            plt.close(fig)\n",
    "            attempt += 1\n",
    "            continue\n",
    "\n",
    "\n",
    "def process_all_frames(single_channel, masks=None):\n",
    "    processed = np.zeros_like(single_channel)\n",
    "    all_masks = np.zeros_like(single_channel, dtype=bool)\n",
    "\n",
    "    for i in range(0,n_frames):\n",
    "        processed[:,:,i], mask = process_single_channel(single_channel[:,:,i].copy(), i)\n",
    "        all_masks[:,:,i] = mask\n",
    "    return processed, all_masks\n",
    "\n",
    "\n",
    "# Process frames\n",
    "\n",
    "\n",
    "processed_red_channel, red_masks = process_all_frames(Red_channel)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261595a2",
   "metadata": {},
   "source": [
    "## Threshold and binarize the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1d9f4b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:24.663627Z",
     "start_time": "2025-05-20T22:41:16.884002Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.signal import convolve2d\n",
    "from matplotlib.widgets import Cursor\n",
    "plt.rc(\"axes\",grid=False)\n",
    "\n",
    "smoothed_image_red = np.zeros_like(processed_red_channel)\n",
    "\n",
    "kernel = np.ones((3, 3)) / 9.0\n",
    "\n",
    "for channel in range(smoothed_image_red.shape[2]):\n",
    "    smoothed_image_red[:, :, channel] = convolve2d(processed_red_channel[:, :, channel], kernel, mode='same', boundary='fill', fillvalue=0)\n",
    "\n",
    "threshold = 2600\n",
    "\n",
    "thresholded_combined = np.where(smoothed_image_red > threshold, 1, 0)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "\n",
    "\n",
    "ax1 = plt.subplot(221)\n",
    "im1 = plt.imshow(processed_red_channel[:, :, 0], cmap='viridis')\n",
    "plt.title('Processed Red Channel - First Frame')\n",
    "\n",
    "\n",
    "ax2 = plt.subplot(222)\n",
    "im2 = plt.imshow(smoothed_image_red[:, :, 0], cmap='viridis')\n",
    "plt.title('Smoothed Red Channel - First Frame')\n",
    "\n",
    "\n",
    "ax3 = plt.subplot(223)\n",
    "im3 = plt.imshow(thresholded_combined[:, :, 0], cmap='viridis')\n",
    "plt.title('Thresholded Red Channel - First Frame')\n",
    "\n",
    "\n",
    "ax4 = plt.subplot(224)\n",
    "im4 = plt.imshow(enhance_contrast(DIC_channel[:, :, 0]), cmap='gray')\n",
    "plt.title('DIC Channel - First Frame')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show(block=True)\n",
    "plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2667a7fc",
   "metadata": {},
   "source": [
    "## Process the binary images with morphological operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7c553",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:25.494998Z",
     "start_time": "2025-05-20T22:41:24.731302Z"
    }
   },
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "from skimage.morphology import binary_erosion, disk, binary_dilation\n",
    "from skimage.filters import median\n",
    "\n",
    "\n",
    "eroded = np.zeros_like(thresholded_combined)\n",
    "despecked = np.zeros_like(thresholded_combined)\n",
    "filled = np.zeros_like(thresholded_combined)\n",
    "dilated = np.zeros_like(thresholded_combined)\n",
    "mask_image_update = np.zeros_like(thresholded_combined)\n",
    "\n",
    "\n",
    "for frame in range(thresholded_combined.shape[2]):\n",
    "\n",
    "    # Fill holes in the image\n",
    "    filled[:, :, frame] = ndimage.binary_fill_holes(thresholded_combined[:, :, frame])\n",
    "\n",
    "    # Erode the image\n",
    "    eroded[:, :, frame] = binary_erosion(filled[:, :, frame], disk(2))\n",
    "\n",
    "    # Despeckle the image using median filter\n",
    "    despecked[:, :, frame] = median(eroded[:, :, frame], disk(1))\n",
    "\n",
    "    # Dilate the image\n",
    "    dilated[:, :, frame] = ndimage.binary_dilation(despecked[:, :, frame], disk(2))\n",
    "\n",
    "    # Update mask image\n",
    "    mask_image_update[:, :, frame] = dilated[:, :, frame]\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(mask_image_update[:,:,0], cmap='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddbf53fb",
   "metadata": {},
   "source": [
    "## Save the thresholded image as final mask of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b5b425",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:25.605983Z",
     "start_time": "2025-05-20T22:41:25.548183Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "full_path_mask = os.path.join(Path_to_Save, 'Thesholded_mask.npy')\n",
    "np.save(full_path_mask, mask_image_update)\n",
    "\n",
    "mask_image_final_8bit = (mask_image_update * 255).astype(np.uint8)\n",
    "\n",
    "mask_save_path = os.path.join(Path_to_Save, 'mask_8bit.tif')\n",
    "tifffile.imwrite(mask_save_path, mask_image_final_8bit.transpose(2,0,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8266af2a",
   "metadata": {},
   "source": [
    "## If you need to load the saved mask later, use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36904558",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:25.674060Z",
     "start_time": "2025-05-20T22:41:25.653058Z"
    }
   },
   "outputs": [],
   "source": [
    "full_path_mask = os.path.join(Path_to_Save, 'Thresholded_mask.npy')\n",
    "mask_save_path = os.path.join(Path_to_Save, 'mask_8bit.tif')\n",
    "\n",
    "mask_image_load = tifffile.imread(mask_save_path)\n",
    "\n",
    "mask_image_reshape = mask_image_load.transpose(1, 2, 0)\n",
    "mask_image_final = mask_image_reshape.astype(np.int64)/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "832aac0f77369929",
   "metadata": {},
   "source": [
    "## Find the centroid of the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bb8c9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:25.855345Z",
     "start_time": "2025-05-20T22:41:25.716833Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find the centroid of the mask\n",
    "\n",
    "from skimage.measure import regionprops, label\n",
    "\n",
    "def find_centroids(binary_image):\n",
    "    \"\"\"Find centroids in all frames of a binary 3D image.\"\"\"\n",
    "    num_frames = binary_image.shape[2]\n",
    "    all_centroids = np.zeros((num_frames, 2))\n",
    "    \n",
    "    for frame in range(num_frames):\n",
    "        # Get current frame\n",
    "        current_frame = binary_image[:,:,frame]\n",
    "        \n",
    "        # Label connected components\n",
    "        labeled_frame = label(current_frame)\n",
    "        \n",
    "        # Get properties of labeled regions\n",
    "        regions = regionprops(labeled_frame)\n",
    "        \n",
    "        # Extract centroids for the largest region in this frame\n",
    "        if regions:\n",
    "            largest_region = max(regions, key=lambda r: r.area)\n",
    "            centroid = largest_region.centroid\n",
    "            all_centroids[frame] = (centroid[1], centroid[0])  # (x, y) coordinates\n",
    "    \n",
    "    return all_centroids\n",
    "\n",
    "# Apply to thresholded image\n",
    "centroids = find_centroids(mask_image_final)\n",
    "\n",
    "# Optional: Plot results for verification\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(mask_image_final[:,:,0], cmap='gray')\n",
    "plt.plot(centroids[0, 0], centroids[0, 1], 'r+')\n",
    "plt.title('Detected Centroids (Frame 0)')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13065fa769cd08f3",
   "metadata": {},
   "source": [
    "## Plot more results for further verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c6cc0c1448e80d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:26.070324Z",
     "start_time": "2025-05-20T22:41:25.911030Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(mask_image_final[:,:,4], cmap='gray')\n",
    "plt.plot(centroids[4, 0], centroids[4, 1], 'r+')\n",
    "plt.title('Detected Centroids')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29981c546e52bf55",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:26.154642Z",
     "start_time": "2025-05-20T22:41:26.149304Z"
    }
   },
   "outputs": [],
   "source": [
    "print(centroids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf5323a48c9014e8",
   "metadata": {},
   "source": [
    "## Save the centroid data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78351ccfb65f3cb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:26.251369Z",
     "start_time": "2025-05-20T22:41:26.215341Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Save the centroid data to an Excel file\n",
    "centroid_df = pd.DataFrame(centroids, columns=['X', 'Y'])\n",
    "excel_path = os.path.join(Path_to_Save, 'centroids.xlsx')\n",
    "centroid_df.to_excel(excel_path, index=False)\n",
    "\n",
    "print(f\"Centroid data saved to {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587f6a185ee8e700",
   "metadata": {},
   "source": [
    "## Load the centroid data and do necessary calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6520db27c10efbb7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:26.980376Z",
     "start_time": "2025-05-20T22:41:26.913836Z"
    }
   },
   "outputs": [],
   "source": [
    "excel_path = os.path.join(Path_to_Save, 'centroids.xlsx')\n",
    "centroid_df_loaded = pd.read_excel(excel_path)\n",
    "centroids = centroid_df_loaded.to_numpy()\n",
    "\n",
    "\n",
    "pixel_width = 0.2767553 # pixel width \n",
    "centroids_adjusted = centroids - centroids[0]\n",
    "centroids_adjusted_scaled = centroids_adjusted * pixel_width\n",
    "\n",
    "Path_to_centroid_Save = os.path.join(Parent_Dir, \"..\", \"Centroids\")\n",
    "excel_path_centroid = os.path.join(Path_to_centroid_Save, full_path_to_load.split('/')[-1].replace('.tif', '.xlsx'))\n",
    "\n",
    "os.makedirs(Path_to_centroid_Save, exist_ok=True)\n",
    "\n",
    "centroid_adjusted_scaled_df = pd.DataFrame(centroids_adjusted_scaled, columns=['X', 'Y'])\n",
    "centroid_adjusted_scaled_df.to_excel(excel_path_centroid, index=False)\n",
    "\n",
    "print(f\"Centroid data saved to {excel_path_centroid}\")\n",
    "\n",
    "\n",
    "distances = np.sqrt(np.sum(np.diff(centroids_adjusted_scaled, axis=0)**2, axis=1))\n",
    "\n",
    "time_interval = 2 # time interval in minutes [adjust if required in seconds]\n",
    "velocity = distances/time_interval\n",
    "\n",
    "\n",
    "average_velocity = np.mean(velocity)\n",
    "\n",
    "Path_to_velocity_Save = os.path.join(Parent_Dir, \"..\", \"Velocities\")\n",
    "os.makedirs(Path_to_velocity_Save, exist_ok=True)\n",
    "\n",
    "excel_path_velocity = os.path.join(Path_to_velocity_Save, full_path_to_load.split('/')[-1].replace('.tif', '.xlsx'))\n",
    "\n",
    "average_velocity_df = pd.DataFrame([average_velocity], columns=['Average Velocity'])\n",
    "average_velocity_df.to_excel(excel_path_velocity, index=False)\n",
    "\n",
    "print(f\"Velocity data saved to {excel_path_velocity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c98c048e344e89",
   "metadata": {},
   "source": [
    "## Plot the centroid data trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af482a6154780315",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-23T01:56:28.353706Z",
     "start_time": "2025-05-23T01:56:28.079182Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "Path_to_centroid_Save = os.path.join(Parent_Dir, \"..\", \"Centroids\")\n",
    "excel_path_centroid = os.path.join(Path_to_centroid_Save, full_path_to_load.split('/')[-1].replace('.tif', '.xlsx'))\n",
    "\n",
    "# Load all the centroid data files from the specified directory\n",
    "centroid_files_before = sorted(glob.glob(os.path.join(Path_to_centroid_Save, '*-before.xlsx')))\n",
    "centroid_files_after = sorted(glob.glob(os.path.join(Path_to_centroid_Save,  '*-after.xlsx')))\n",
    "\n",
    "\n",
    "# Initialize an empty DataFrame to store all the stacked data\n",
    "stacked_centroid_before_df = pd.DataFrame()\n",
    "stacked_centroid_after_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and stack the data\n",
    "for file in centroid_files_before:\n",
    "    centroid_df_before = pd.read_excel(file)\n",
    "    stacked_centroid_before_df = pd.concat([stacked_centroid_before_df, centroid_df_before[['X', 'Y']]], axis=1)\n",
    "\n",
    "# print(stacked_centroid_before_df)\n",
    "\n",
    "stacked_centroid_before_df.to_excel(os.path.join(Parent_Dir, '..', 'stacked_centroids_before.xlsx'), index=False)\n",
    "\n",
    "stacked_centroid_before_array = stacked_centroid_before_df.to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "colors = plt.cm.tab20.colors + plt.cm.tab20b.colors  # Combines two colormaps for more distinct colors\n",
    "\n",
    "for i in range(0, stacked_centroid_before_array.shape[1], 2):\n",
    "    color_idx = i // 2\n",
    "    plt.plot(stacked_centroid_before_array[:, i], stacked_centroid_before_array[:, i + 1],\n",
    "             color=colors[color_idx % len(colors)])\n",
    "    \n",
    "plt.legend()\n",
    "plt.title('Before')\n",
    "plt.axhline(0, ls = '--', color = 'k', alpha = 0.4, lw = 1)\n",
    "plt.axvline(0, ls = '--', color = 'k', alpha = 0.4, lw = 1)\n",
    "plt.axis([-250, 250, -250, 250])\n",
    "\n",
    "\n",
    "for file in centroid_files_after:\n",
    "    centroid_df_after = pd.read_excel(file)\n",
    "    stacked_centroid_after_df = pd.concat([stacked_centroid_after_df, centroid_df_after[['X', 'Y']]], axis=1)\n",
    "\n",
    "print(stacked_centroid_before_df)\n",
    "print(stacked_centroid_after_df)\n",
    "\n",
    "stacked_centroid_after_df.to_excel(os.path.join(Parent_Dir, '..', 'stacked_centroids_after.xlsx'), index=False)\n",
    "\n",
    "stacked_centroid_after_array = stacked_centroid_after_df.to_numpy()\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "colors = plt.cm.tab20.colors + plt.cm.tab20b.colors\n",
    "for i in range(0, stacked_centroid_after_array.shape[1], 2):\n",
    "    color_idx = i // 2\n",
    "    plt.plot(stacked_centroid_after_array[:, i], stacked_centroid_after_array[:, i + 1],\n",
    "             color=colors[color_idx % len(colors)])\n",
    "plt.legend()\n",
    "plt.title('After')\n",
    "plt.axhline(0, ls = '--', color = 'k', alpha = 0.4, lw = 1)\n",
    "plt.axvline(0, ls = '--', color = 'k', alpha = 0.4, lw = 1)\n",
    "plt.axis([-210, 210, -210, 210])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c161fd14658bf629",
   "metadata": {},
   "source": [
    "## Plot the velocity data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b25048a2f034ba01",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:28.038237Z",
     "start_time": "2025-05-20T22:41:27.778851Z"
    }
   },
   "outputs": [],
   "source": [
    "Path_to_velocity_Save = os.path.join(Parent_Dir, \"..\", \"Velocities\")\n",
    "\n",
    "# Load all the velocity data files from the specified directory\n",
    "velocity_files_before = sorted(glob.glob(os.path.join(Path_to_velocity_Save, '*-before.xlsx')))\n",
    "velocity_files_after = sorted(glob.glob(os.path.join(Path_to_velocity_Save, '*-after.xlsx')))\n",
    "\n",
    "velocity_before_df = pd.DataFrame()\n",
    "velocity_after_df = pd.DataFrame()\n",
    "\n",
    "# Loop through each file and stack the data\n",
    "for file in velocity_files_before:\n",
    "    velocity_df_before = pd.read_excel(file)\n",
    "    velocity_df_before['Filename'] = os.path.basename(file)\n",
    "    velocity_before_df = pd.concat([velocity_before_df, velocity_df_before], axis=0)\n",
    "\n",
    "velocity_before_df.to_excel(os.path.join(Parent_Dir, \"..\", \"Velocities\", \"1_Combined_Before.xlsx\"), index=False)\n",
    "\n",
    "for file in velocity_files_after:\n",
    "    velocity_df_after = pd.read_excel(file)\n",
    "    velocity_df_after['Filename'] = os.path.basename(file)\n",
    "    velocity_after_df = pd.concat([velocity_after_df, velocity_df_after], axis=0)\n",
    "\n",
    "\n",
    "velocity_after_df.to_excel(os.path.join(Parent_Dir, \"..\", \"Velocities\", \"1_Combined_After.xlsx\"), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2d71b3c8fba00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:28.462488Z",
     "start_time": "2025-05-20T22:41:28.116285Z"
    }
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"ticks\", {'axes.grid' : False})\n",
    "\n",
    "plt.figure(figsize=(6, 8))\n",
    "\n",
    "# Create DataFrame with paired data\n",
    "data = pd.DataFrame({\n",
    "    'Velocity': pd.concat([velocity_before_df['Average Velocity'], velocity_after_df['Average Velocity']]),\n",
    "    'Condition': ['Before'] * len(velocity_before_df) + ['After'] * len(velocity_after_df)\n",
    "})\n",
    "\n",
    "data_visible = pd.DataFrame({\n",
    "    'Velocity': pd.concat([velocity_before_df['Average Velocity'], velocity_after_df['Average Velocity']]),\n",
    "    'Condition': ['Before'] * len(velocity_before_df) + ['After'] * len(velocity_after_df),\n",
    "    'Plotting': ['Visible'] * (len(velocity_before_df) + len(velocity_after_df))\n",
    "})\n",
    "\n",
    "data_invisible = pd.DataFrame({\n",
    "    'Velocity': pd.concat([velocity_before_df['Average Velocity'], velocity_after_df['Average Velocity']]),\n",
    "    'Condition': ['Before'] * len(velocity_before_df) + ['After'] * len(velocity_after_df),\n",
    "    'Plotting': ['Invisible'] * (len(velocity_before_df) + len(velocity_after_df))\n",
    "})\n",
    "\n",
    "combined_data_visible = pd.concat([data_visible, data_invisible])\n",
    "flierprops = {'marker': 'o', 'markerfacecolor': 'none', 'markeredgecolor': 'm', 'markersize': 9, 'linestyle': 'none'}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define custom colors\n",
    "palet = sns.color_palette([\"lightsteelblue\", \"white\"])\n",
    "\n",
    "\n",
    "# Create half violin plot\n",
    "sns.violinplot(x='Condition', y='Velocity', hue='Plotting', data=combined_data_visible,\n",
    "               split=True, inner=None, palette=palet, linewidth=0, alpha=0.59, legend=False)\n",
    "sns.boxplot(x='Condition', y='Velocity', data=data, width=0.2, linewidth=1.8, palette=\"Paired\", flierprops=flierprops)\n",
    "sns.stripplot(x='Condition', y='Velocity', data=data, color='k', size=6, alpha=0.7, jitter=False)\n",
    "sns.despine()\n",
    "\n",
    "# Add lines connecting before and after points\n",
    "before_points = velocity_before_df['Average Velocity'].values\n",
    "after_points = velocity_after_df['Average Velocity'].values\n",
    "\n",
    "for b, a in zip(before_points, after_points):\n",
    "    plt.plot([0, 1], [b, a], color='gray', alpha=0.9, linestyle='solid', linewidth=1.2)\n",
    "\n",
    "# plt.title('Velocity Distribution')\n",
    "plt.xlabel(\"\")\n",
    "plt.ylabel('Velocity (µm/min)', fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.tight_layout()\n",
    "# plt.draw()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f569abb779b101",
   "metadata": {},
   "source": [
    "## Do the Statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae01f7ecf3078799",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:41:35.426299Z",
     "start_time": "2025-05-20T22:41:35.403483Z"
    },
    "tags": [
     "Run_All_Above"
    ]
   },
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "# Perform Mann-Whitney U test\n",
    "statistic_mw, p_value_mw = stats.mannwhitneyu(\n",
    "    velocity_before_df['Average Velocity'],\n",
    "    velocity_after_df['Average Velocity'],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "# Perform Wilcoxon signed-rank test\n",
    "statistic_w, p_value_w = stats.wilcoxon(\n",
    "    velocity_before_df['Average Velocity'],\n",
    "    velocity_after_df['Average Velocity'],\n",
    "    alternative='two-sided'\n",
    ")\n",
    "\n",
    "print(\"Statistical Test Results:\")\n",
    "print(\"\\nMann-Whitney U test:\")\n",
    "print(f\"Statistic: {statistic_mw}\")\n",
    "print(f\"P-value: {p_value_mw}\")\n",
    "\n",
    "print(\"\\nWilcoxon signed-rank test:\")\n",
    "print(f\"Statistic: {statistic_w}\")\n",
    "print(f\"P-value: {p_value_w}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4ec51fc247922b",
   "metadata": {},
   "source": [
    "## Plot all the velocity data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f3fd0706f86fc87",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:42:13.713056Z",
     "start_time": "2025-05-20T22:42:13.586318Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load all the velocity data files from the specified directory\n",
    "base_directory = \"/media/tatsatb/Data/\"\n",
    "\n",
    "Path_to_after_velocity_files = {}\n",
    "Path_to_before_velocity_files = {}\n",
    "\n",
    "\n",
    "for subdir in ['AX2', 'GEFB', 'GEFU', 'GEFX', 'GEFM']:\n",
    "    Path_to_after_velocity_files[subdir] = os.path.join(base_directory, subdir, \"Velocities\", \"1_Combined_After.xlsx\")\n",
    "    Path_to_before_velocity_files[subdir] = os.path.join(base_directory, subdir, \"Velocities\", \"1_Combined_Before.xlsx\")\n",
    "    if not os.path.exists(Path_to_velocity_Save):\n",
    "        print(f\"Directory {Path_to_velocity_Save} does not exist.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "df_velocity_combined_after = pd.DataFrame()\n",
    "df_velocity_combined_before = pd.DataFrame()\n",
    "\n",
    "after_list = []\n",
    "before_list = []\n",
    "\n",
    "# Load each file and add as a column with the key as header\n",
    "for key, path in Path_to_after_velocity_files.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_excel(path)\n",
    "        df[\"Group\"] = key\n",
    "        after_list.append(df[['Average Velocity', 'Group']])\n",
    "\n",
    "    else:\n",
    "        print(f\"File not found: {path}\")\n",
    "\n",
    "\n",
    "for key, path in Path_to_before_velocity_files.items():\n",
    "    if os.path.exists(path):\n",
    "        df = pd.read_excel(path)\n",
    "        df['Group'] = key\n",
    "        before_list.append(df[['Average Velocity', 'Group']])\n",
    "        \n",
    "    else:\n",
    "        print(f\"File not found: {path}\")\n",
    "        continue\n",
    "\n",
    "\n",
    "df_velocity_combined_after = pd.concat(after_list, ignore_index=True)\n",
    "df_velocity_combined_before = pd.concat(before_list, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3339f801b040546",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:42:16.947510Z",
     "start_time": "2025-05-20T22:42:15.593921Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Combine before and after into one DataFrame for plotting\n",
    "\n",
    "df_velocity_combined_before['Condition'] = 'Before'\n",
    "df_velocity_combined_after['Condition'] = 'After'\n",
    "df_all = pd.concat([df_velocity_combined_before, df_velocity_combined_after], ignore_index=True)\n",
    "\n",
    "\n",
    "sns.set_style(\"ticks\", {'axes.grid': False})\n",
    "\n",
    "\n",
    "# Get all unique groups\n",
    "all_groups = sorted(df_all['Group'].unique())\n",
    "\n",
    "for group in all_groups:\n",
    "    # Filter data for this group\n",
    "    group_before = df_velocity_combined_before[df_velocity_combined_before['Group'] == group]\n",
    "    group_after = df_velocity_combined_after[df_velocity_combined_after['Group'] == group]\n",
    "\n",
    "    # Create DataFrame with paired data\n",
    "    data = pd.DataFrame({\n",
    "        'Velocity': pd.concat([group_before['Average Velocity'], group_after['Average Velocity']]),\n",
    "        'Condition': ['Before'] * len(group_before) + ['After'] * len(group_after)\n",
    "    })\n",
    "\n",
    "    data_visible = pd.DataFrame({\n",
    "        'Velocity': pd.concat([group_before['Average Velocity'], group_after['Average Velocity']]),\n",
    "        'Condition': ['Before'] * len(group_before) + ['After'] * len(group_after),\n",
    "        'Plotting': ['Visible'] * (len(group_before) + len(group_after))\n",
    "    })\n",
    "\n",
    "    data_invisible = pd.DataFrame({\n",
    "        'Velocity': pd.concat([group_before['Average Velocity'], group_after['Average Velocity']]),\n",
    "        'Condition': ['Before'] * len(group_before) + ['After'] * len(group_after),\n",
    "        'Plotting': ['Invisible'] * (len(group_before) + len(group_after))\n",
    "    })\n",
    "\n",
    "    combined_data_visible = pd.concat([data_visible, data_invisible])\n",
    "    flierprops = {'marker': 'o', 'markerfacecolor': 'none', 'markeredgecolor': 'm', 'markersize': 9, 'linestyle': 'none'}\n",
    "\n",
    "    palet = sns.color_palette([\"lightsteelblue\", \"white\"])\n",
    "    sns.set_style(\"ticks\", {'axes.grid': False})\n",
    "\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    sns.violinplot(x='Condition', y='Velocity', hue='Plotting', data=combined_data_visible,\n",
    "                   split=True, inner=None, palette=palet, linewidth=0, alpha=0.59, legend=False)\n",
    "    sns.boxplot(x='Condition', y='Velocity', data=data, width=0.2, linewidth=1.8, palette=\"Paired\", flierprops=flierprops)\n",
    "    sns.stripplot(x='Condition', y='Velocity', data=data, color='k', size=6, alpha=0.7, jitter=False)\n",
    "    sns.despine()\n",
    "\n",
    "\n",
    "    # Add lines connecting before and after points (if paired)\n",
    "    before_points = group_before['Average Velocity'].values\n",
    "    after_points = group_after['Average Velocity'].values\n",
    "    for b, a in zip(before_points, after_points):\n",
    "        plt.plot([0, 1], [b, a], color='gray', alpha=0.9, linestyle='solid', linewidth=1.2)\n",
    "\n",
    "\n",
    "    plt.xlabel(\"\")\n",
    "    plt.ylabel('Velocity (µm/min)', fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.title(f'Velocity Distribution: {group}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7227ff483529d075",
   "metadata": {},
   "source": [
    "## Plot the velocity data: All groups together on same axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82608c3f1637cd69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T22:58:31.077986Z",
     "start_time": "2025-05-20T22:58:29.742499Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create DataFrame with paired data\n",
    "data_combined = pd.concat([df_velocity_combined_before, df_velocity_combined_after])\n",
    "\n",
    "data_combined_visible = data_combined.copy()\n",
    "data_combined_visible['Plotting'] = 'Visible'\n",
    "\n",
    "\n",
    "data_combined_invisible = data_combined.copy()\n",
    "data_combined_invisible['Plotting'] = 'Invisible'\n",
    "\n",
    "\n",
    "data_combined_final = pd.concat([data_combined_visible, data_combined_invisible])\n",
    "data_combined_final[\"Group_Condition\"] = data_combined_final['Group'] + '_' + data_combined_final['Condition']\n",
    "\n",
    "\n",
    "\n",
    "# Define custom order for the x-axis\n",
    "groups = data_combined_final['Group'].unique()\n",
    "custom_order = []\n",
    "for group in groups:\n",
    "    custom_order.append(f\"{group}_Before\")\n",
    "    custom_order.append(f\"{group}_After\")\n",
    "\n",
    "flierprops_mod = {'marker': 'o', 'markerfacecolor': 'none', 'markeredgecolor': 'm', 'markersize': 9, 'linestyle': 'none'}\n",
    "\n",
    "# Create the plot with the new x-axis variable and custom order\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.violinplot(x='Group_Condition', y='Average Velocity', hue='Plotting',\n",
    "               data=data_combined_final, width=1.4, split=True, inner=None,  # Add quartile lines\n",
    "               palette=palet, linewidth=0, alpha=0.59, legend=False,\n",
    "               order=custom_order)  # Apply custom order\n",
    "\n",
    "sns.boxplot(x='Group_Condition', y='Average Velocity', data=data_combined_final,\n",
    "            width=0.22, linewidth=1.5, palette=\"Paired\", flierprops=flierprops_mod,\n",
    "            order=custom_order, whis=1.5)  # Add whiskers for 5th and 95th percentiles\n",
    "\n",
    "sns.stripplot(x='Group_Condition', y='Average Velocity',data=data_combined_final,color='k', size=5, alpha=0.7, jitter=False,\n",
    "             order=custom_order)\n",
    "\n",
    "# Now add connecting lines for each group\n",
    "for group in groups:\n",
    "    before_data = data_combined_final[(data_combined_final['Group'] == group) &\n",
    "                                     (data_combined_final['Condition'] == 'Before')]\n",
    "    after_data = data_combined_final[(data_combined_final['Group'] == group) &\n",
    "                                    (data_combined_final['Condition'] == 'After')]\n",
    "\n",
    "    # Find x-positions for this group in the plot\n",
    "    before_idx = custom_order.index(f\"{group}_Before\")\n",
    "    after_idx = custom_order.index(f\"{group}_After\")\n",
    "\n",
    "    # Match points (assuming they're in the same order)\n",
    "    before_points = before_data['Average Velocity'].values\n",
    "    after_points = after_data['Average Velocity'].values\n",
    "\n",
    "    # Draw lines connecting each pair of points\n",
    "    min_length = min(len(before_points), len(after_points))\n",
    "    for i in range(min_length):\n",
    "        plt.plot([before_idx, after_idx], [before_points[i], after_points[i]],\n",
    "                 color='gray', alpha=0.5, linestyle='solid', linewidth=0.8)\n",
    "\n",
    "sns.despine()\n",
    "plt.ylim(-1, 20)\n",
    "plt.xticks(rotation=15)  # Rotate labels for better readability\n",
    "plt.tight_layout()  # Adjust layout to make room for rotated labels\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('velocity_plot.svg', format='svg')\n",
    "plt.savefig('velocity_plot.pdf', format='pdf')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ImageProcess",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
